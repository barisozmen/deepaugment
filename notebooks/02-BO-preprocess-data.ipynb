{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Related to [issue#5](https://github.com/barisozmen/aerial-autoaug/issues/5)\n",
    "\n",
    "Data preprocess should be like:\n",
    "\n",
    "1. Remove images having width or height less than 608*\n",
    "2. Split images using SplitImg.py module of DOTA_devkit, where subsize=608 and gap=0.\n",
    "3. Remove any image whose after-split dimensions are not order of 608\n",
    "4. Convert oriented bounding boxes (OBB) to horizontal bounding boxes (HBB)\n",
    "\n",
    "For pipeline v0.1, only use 20 images for training set, where 10 of them having \"planes\" in it. All images from test set. MVP targets only to detect planes.\n",
    "\n",
    "The rationale of choosing 608 as size is that pre-trained model, which I will use in v0.1, was trained by 608x608 images (https://github.com/ringringyi/DOTA_models#training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import shutil\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from shutil import copytree, ignore_patterns\n",
    "import sys\n",
    "sys.path.insert(1, \"../DOTA_devkit/\")\n",
    "\n",
    "from DOTA import DOTA\n",
    "from ImgSplit import splitbase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOTA_DIR = \"../data/raw/DOTA/\"\n",
    "DOTA_TRAIN_DIR = \"../data/raw/DOTA/training/\"\n",
    "DOTA_VAL_DIR = \"../data/raw/DOTA/validation/\"\n",
    "DOTA_MVP_DIR = \"../data/raw/DOTA_MVP/\"\n",
    "\n",
    "IMAGE_DIM = 300\n",
    "RESIZE_FACTOR = 0.25\n",
    "\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0-a. Delete neccessary directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"../data/raw/DOTA_MVP/\"):\n",
    "    shutil.rmtree(\"../data/raw/DOTA_MVP/\")\n",
    "if os.path.exists(\"../data/processed/DOTA_MVP/\"):\n",
    "    shutil.rmtree(\"../data/processed/DOTA_MVP/\")\n",
    "if os.path.exists(\"../data/processed/DOTA_MVP_splitted/\"):\n",
    "    shutil.rmtree(\"../data/processed/DOTA_MVP_splitted/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0-b. Make MVP-DOTA directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(DOTA_MVP_DIR):\n",
    "    print (f\"{DOTA_MVP_DIR} already exist, dont make MVP-DOTA directory again\")\n",
    "else:\n",
    "    \n",
    "    from shutil import copyfile\n",
    "    \n",
    "    def copy_image_by_id(image_id, from_dir, to_dir):\n",
    "        # copy image file\n",
    "        image_file = f\"{image_id}.png\"\n",
    "        from_path = from_dir + \"images/\" + image_file\n",
    "        to_path = to_dir + \"images/\" + image_file\n",
    "        copyfile(from_path, to_path)\n",
    "        # copy label file\n",
    "        label_file = f\"{image_id}.txt\"\n",
    "        from_path = from_dir + \"labelTxt/\" + label_file\n",
    "        to_path = to_dir + \"labelTxt/\" + label_file\n",
    "        copyfile(from_path, to_path)\n",
    "        \n",
    "    dota_training = DOTA(DOTA_TRAIN_DIR)\n",
    "    dota_validation = DOTA(DOTA_VAL_DIR)\n",
    "    \n",
    "    os.mkdir( DOTA_MVP_DIR )\n",
    "    for split_set in (\"/training/\", \"/validation/\"):\n",
    "        os.mkdir( DOTA_MVP_DIR + split_set)\n",
    "        os.mkdir( DOTA_MVP_DIR + split_set + \"images/\" )\n",
    "        os.mkdir( DOTA_MVP_DIR + split_set + \"labelTxt/\" )\n",
    "    \n",
    "    ################################################################\n",
    "    # Copy 10 plane and 10 non-plane images from training set\n",
    "    ################################################################\n",
    "    \n",
    "    all_tr_image_ids = dota_training.getImgIds()\n",
    "    plane_tr_image_ids = dota_training.getImgIds(catNms=['plane'])\n",
    "    non_plane_tr_image_ids = list(set(all_tr_image_ids).difference(plane_tr_image_ids))\n",
    "\n",
    "    selected_plane_tr_image_ids = np.random.choice(plane_tr_image_ids,150)\n",
    "    selected_nonplane_tr_image_ids = np.random.choice(non_plane_tr_image_ids,150)\n",
    "    \n",
    "    for image_id in np.concatenate([selected_plane_tr_image_ids, selected_nonplane_tr_image_ids]):\n",
    "        copy_image_by_id(\n",
    "            image_id = image_id,\n",
    "            from_dir = DOTA_DIR + \"training/\",\n",
    "            to_dir = DOTA_MVP_DIR + \"training/\"\n",
    "        )\n",
    "        \n",
    "    ################################################################\n",
    "    # Copy 100 images from validatation set\n",
    "    ################################################################\n",
    "    \n",
    "    all_val_image_ids = dota_validation.getImgIds(catNms=['plane'])\n",
    "    selected_val_image_ids = np.random.choice(all_val_image_ids, 2)\n",
    "    \n",
    "    for image_id in selected_val_image_ids:\n",
    "        copy_image_by_id(\n",
    "            image_id = image_id,\n",
    "            from_dir = DOTA_DIR + \"validation/\",\n",
    "            to_dir = DOTA_MVP_DIR + \"validation/\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy whole library to /data/processed/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_DIR = \"../data/processed/DOTA_MVP\"\n",
    "\n",
    "assert not os.path.exists(TARGET_DIR), 'data already preprocessed'\n",
    "\n",
    "copytree(DOTA_MVP_DIR, TARGET_DIR, ignore=ignore_patterns('*.pyc', 'tmp*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = DOTA(TARGET_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Remove images having width or height less than 300. Otherwise resize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for folder_set in [\"training\", \"validation\"]:\n",
    "    for image_name in os.listdir(TARGET_DIR + f\"/{folder_set}/images/\"):\n",
    "        image_path = TARGET_DIR + f\"/{folder_set}/images/\" + image_name\n",
    "        im = Image.open(image_path)\n",
    "\n",
    "        if im.size[0]<IMAGE_DIM or im.size[1]<IMAGE_DIM:\n",
    "            os.remove(image_path)\n",
    "            print (f\"image at {image_path} removed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Split images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLITTED_TARGET_DIR = \"../data/processed/DOTA_MVP_splitted\"\n",
    "\n",
    "for folder_set in [\"training\", \"validation\"]:\n",
    "    split = splitbase(\n",
    "        TARGET_DIR + \"/\" + folder_set, \n",
    "        SPLITTED_TARGET_DIR + \"/\" + folder_set, \n",
    "        choosebestpoint = True,\n",
    "        subsize = IMAGE_DIM,\n",
    "        gap=0\n",
    "    )\n",
    "    split.splitdata(RESIZE_FACTOR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Remove images whose dimensions are not on the order of 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "removed=[]\n",
    "\n",
    "for folder_set in [\"training\", \"validation\"]:\n",
    "    for image_name in os.listdir(SPLITTED_TARGET_DIR + f\"/{folder_set}/images/\"):\n",
    "        image_path = SPLITTED_TARGET_DIR + f\"/{folder_set}/images/\" + image_name\n",
    "        im = Image.open(image_path)\n",
    "        \n",
    "        if im.size[0]!=IMAGE_DIM or im.size[1]!=IMAGE_DIM:\n",
    "            os.remove(image_path)\n",
    "            print (f\"image at {image_path} removed\")\n",
    "            \n",
    "        removed.append(image_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Convert labels to Udacity driving dataset labels format\n",
    "\n",
    "As here:\n",
    "https://raw.githubusercontent.com/udacity/self-driving-car/master/annotations/labels_crowdai.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder_set in [\"training\", \"validation\"]:\n",
    "    labels_path = SPLITTED_TARGET_DIR + f\"/{folder_set}/labelTxt/\"\n",
    "\n",
    "    all_labels_df = pd.DataFrame()\n",
    "\n",
    "    for label_name in os.listdir(labels_path):\n",
    "        \n",
    "        if label_name.replace(\".txt\",\"\") in [x.replace(\".png\",\"\") for x in removed]:\n",
    "            continue\n",
    "\n",
    "        label_path = labels_path + label_name\n",
    "\n",
    "        label_df = pd.read_csv(\n",
    "            label_path,\n",
    "            sep=\" \", header=None,\n",
    "            names = [\"x1\",\"y1\",\"x2\",\"y2\",\"x3\",\"y3\",\"x4\",\"y4\",\"category\",\"difficulty\"]\n",
    "        )\n",
    "\n",
    "        new_label_df = pd.DataFrame(columns=[\"image_name\",\"xmin\",\"xmax\",\"ymin\",\"ymax\",\"class_id\"])\n",
    "        \n",
    "        new_label_df[\"xmin\"] = label_df[[\"x1\",\"x2\",\"x3\",\"x4\"]].min(axis=1).astype(int)\n",
    "        new_label_df[\"xmax\"] = label_df[[\"x1\",\"x2\",\"x3\",\"x4\"]].max(axis=1).astype(int)\n",
    "        new_label_df[\"ymin\"] = label_df[[\"y1\",\"y2\",\"y3\",\"y4\"]].min(axis=1).astype(int)\n",
    "        new_label_df[\"ymax\"] = label_df[[\"y1\",\"y2\",\"y3\",\"y4\"]].max(axis=1).astype(int)\n",
    "        new_label_df[\"category\"] = label_df[\"category\"]\n",
    "        new_label_df[\"image_name\"] = label_name.split(\".\")[0] + \"_\" +label_name.split(\".\")[1]+\".png\"\n",
    "        \n",
    "#         for col in [\"xmin\",\"xmax\",\"ymin\",\"ymax\"]:\n",
    "#             new_label_df[col] = new_label_df[col]/RESIZE_FACTOR\n",
    "        \n",
    "        # Remove all whose class is not plane\n",
    "        new_label_df = new_label_df[new_label_df[\"category\"]==\"plane\"]\n",
    "        \n",
    "        new_label_df = new_label_df[new_label_df[\"image_name\"].isin([x.replace(\".\",\"_\") for x in removed])==False]\n",
    "        \n",
    "        # Only one class: plane\n",
    "        new_label_df[\"class_id\"]=1\n",
    "        \n",
    "        ORDER = [\"image_name\", \"xmin\", \"xmax\", \"ymin\", \"ymax\", \"class_id\"]\n",
    "        new_label_df = new_label_df[ORDER]\n",
    "        all_labels_df = pd.concat([all_labels_df, new_label_df])\n",
    "\n",
    "    all_labels_df.to_csv(SPLITTED_TARGET_DIR + f\"/{folder_set}_labels.csv\", sep=\",\", index=False)\n",
    "    \n",
    "# TODO merge training and validation images somewhere"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge training and validation folders into \"all_images\" folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recursively merge two folders including subfolders\n",
    "def mergefolders(root_src_dir, root_dst_dir):\n",
    "    for src_dir, dirs, files in os.walk(root_src_dir):\n",
    "        dst_dir = src_dir.replace(root_src_dir, root_dst_dir, 1)\n",
    "        if not os.path.exists(dst_dir):\n",
    "            os.makedirs(dst_dir)\n",
    "        for file_ in files:\n",
    "            src_file = os.path.join(src_dir, file_)\n",
    "            dst_file = os.path.join(dst_dir, file_)\n",
    "            if os.path.exists(dst_file):\n",
    "                os.remove(dst_file)\n",
    "            shutil.copy(src_file, dst_dir)\n",
    "            \n",
    "mergefolders(\n",
    "    SPLITTED_TARGET_DIR + f\"/training/images/\",\n",
    "    SPLITTED_TARGET_DIR + f\"/all_images/\"\n",
    ")\n",
    "mergefolders(\n",
    "    SPLITTED_TARGET_DIR + f\"/validation/images/\",\n",
    "    SPLITTED_TARGET_DIR + f\"/all_images/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rename all by removing dots in the middle\n",
    "e.g. 'P0032__0.25__300___0.png' to 'P0032__0_25__300___0.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLITTED_TARGET_DIR + f\"/all_images/\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
